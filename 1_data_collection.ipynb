{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step implementation plan \n",
    "\n",
    "1.  Data collection\n",
    "\n",
    "    1.  Get tickers from S&P 500 on Wikipedia (see [5 Lines of Python to Automate Getting the S&P 500](https://medium.com/wealthy-bytes/5-lines-of-python-to-automate-getting-the-s-p-500-95a632e5e567) for inspiration)\n",
    "\n",
    "    2.  Get main outcome: ESG risk, obtained through the yahooquery Pypi package (accessed with the parameter esg_scores for a ticker)\n",
    "\n",
    "    3.  Obtain search metrics on ESG related keywords from Google trends through the pytrends Pypi package \n",
    "\n",
    "3.  Feature transformation\n",
    "\n",
    "    1.  Collapse time dimension of Google trends search index into the following metrics for each keyword, using a defined time span, such as one year\n",
    "\n",
    "        1.  Maximum search index \n",
    "\n",
    "        2.  Avg. search index \n",
    "\n",
    "    3.  Possibly Impute missing with mean\n",
    "\n",
    "    4.  Split data into a train and test set, convert to .csv and upload to S3\n",
    "\n",
    "5.  Descriptive statistics\n",
    "\n",
    "    1.  Pick ten firms and show feature averages in a table\n",
    "\n",
    "    2.  Correlation matrix of features and outcome\n",
    "\n",
    "    3.  Create a pairplot\n",
    "\n",
    "7.  Training, validating and testing a model with Sagemaker\n",
    "\n",
    "    1.  Write scripts for benchmark linear regression: train.py\n",
    "\n",
    "    2.  Write scripts for PyTorch neural network: model.py and train.py\n",
    "\n",
    "    3.  Instantiate estimators with Sagemaker \n",
    "\n",
    "    4.  Run training job\n",
    "\n",
    "    5.  Deploy models for testing\n",
    "\n",
    "9.  Evaluation and benchmark comparison \n",
    "\n",
    "    1.  RMSE and R-squared\n",
    "\n",
    "    2.  Possible model adjustment of the neural net when it lacks precision\n",
    "\n",
    "11. Cleanup\n",
    "\n",
    "    1.  Delete endpoint\n",
    "\n",
    "    2.  Remove other resources, such as training jobs, endpoint configurations, notebook instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yahooquery in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas>=0.24 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from yahooquery) (1.0.3)\n",
      "Requirement already satisfied: selenium==3.141.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from yahooquery) (3.141.0)\n",
      "Requirement already satisfied: requests-futures==1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from yahooquery) (1.0.0)\n",
      "Requirement already satisfied: lxml==4.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from yahooquery) (4.5.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.24->yahooquery) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.24->yahooquery) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.24->yahooquery) (2.8.1)\n",
      "Requirement already satisfied: urllib3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from selenium==3.141.0->yahooquery) (1.25.8)\n",
      "Requirement already satisfied: requests>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-futures==1.0.0->yahooquery) (2.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.24->yahooquery) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=1.2.0->requests-futures==1.0.0->yahooquery) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=1.2.0->requests-futures==1.0.0->yahooquery) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=1.2.0->requests-futures==1.0.0->yahooquery) (2.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install required packages \n",
    "!pip install yahooquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytrends in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.7.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytrends) (2.23.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytrends) (1.0.3)\n",
      "Requirement already satisfied: lxml in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytrends) (4.5.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytrends) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytrends) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytrends) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytrends) (2020.4.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.25->pytrends) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.25->pytrends) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.25->pytrends) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.25->pytrends) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from math import ceil\n",
    "from math import exp\n",
    "import os\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# APIs for collecting data \n",
    "from yahooquery import Ticker\n",
    "from pytrends.request import TrendReq \n",
    "from time import sleep # avoid too many requests error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "#### Version info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.10 :: Anaconda, Inc.\n",
      "Pandas version 1.0.3\n",
      "Pytorch version\n"
     ]
    }
   ],
   "source": [
    "py_version = !python --version\n",
    "print(py_version[0])\n",
    "print(\"Pandas version\",pd.__version__)\n",
    "print(\"Pytorch version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sagemaker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# role\n",
    "\n",
    "\n",
    "# Session\n",
    "# S3 bucket\n",
    "# folder prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (505, 9)\n",
      "\n",
      "TICKER EXAMPLE:\n",
      "['MMM', 'ABT', 'ABBV']\n"
     ]
    }
   ],
   "source": [
    "# retrieve S&P 500 listings from Wikipedia\n",
    "table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "df_sp500 = table[0]\n",
    "\n",
    "print(\"SHAPE:\",df_sp500.shape)\n",
    "\n",
    "# TICKER\n",
    "ticker = list(df_sp500.Symbol)\n",
    "print(\"\\nTICKER EXAMPLE:\")\n",
    "print(ticker[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yahooquery: ESG ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      totalEsg  environmentScore  socialScore  governanceScore\n",
      "MMM      34.75             12.79        13.89             8.07\n",
      "ABT      29.83              2.98        16.21            10.63\n",
      "ABBV     30.19              0.96        16.53            12.69\n",
      "ACN      11.24              0.55         4.98             5.71\n",
      "ATVI     16.71              0.15        10.46             6.09\n",
      "\n",
      "Focus on totalEsg score:\n",
      "[34.75 29.83 30.19 11.24 16.71 13.59 12.38 33.15 23.62 16.79 11.24 21.16\n",
      " 36.72 24.69 36.02 22.63 29.62 24.82 30.57 40.24 30.59 34.92 22.13 32.12\n",
      " 28.99 27.87 17.78 29.46 20.43 23.81 16.7  23.   32.05 39.05 18.66 23.62\n",
      " 15.01 34.24 19.35 24.72 26.02 18.71 35.63 12.4  11.99 12.68 18.26 16.2\n",
      " 27.32 21.49 27.97 31.74 11.85 21.56 23.43 23.19 38.98 19.45 23.99 14.35\n",
      " 30.79 28.19 26.32 19.34 25.96 49.96 22.   26.55 15.64 13.5  21.72 37.02\n",
      " 21.69  8.64 29.85 28.69 31.55 25.48 20.1  47.74 26.89 20.   40.22 24.74\n",
      " 24.82 25.23 26.16 25.35 12.08 28.59 27.92 18.09 23.5  27.97 26.23 21.67\n",
      " 21.82 19.65 26.63 32.85 51.7  29.06 26.96 24.15 22.13 23.41 19.52 22.84\n",
      " 31.53 20.26 22.57 19.1  20.7  21.07 32.72 22.18 20.66 17.77 23.82 20.48\n",
      " 33.25 27.15 14.42 23.54 18.14 21.65 17.01 19.52 32.25 26.68 33.05 30.89\n",
      " 33.26 33.01 15.32 24.76 16.87 21.42 30.54 28.85 19.65 12.44 30.82 34.89\n",
      " 46.76 28.24 15.84 15.9  25.34 22.83 28.05 24.29 23.41 15.74 32.23 31.37\n",
      " 22.53 16.1  21.47 18.68 25.88 37.93 24.54 19.49 19.59 38.09 38.89 30.\n",
      " 21.39 30.94 20.   41.82 13.35 20.69 28.86 37.37 45.84 24.84 31.25 12.97\n",
      " 22.45 26.51 22.2  31.06 13.02 26.51 13.93 20.55 12.56 26.58 10.5  14.88\n",
      " 25.27 29.38 17.35 20.51 36.64 30.58 13.44 33.42 24.45 11.7  22.89 22.66\n",
      " 33.72 24.1  20.65 11.16 27.73 20.68 15.49 15.97 17.58 25.68 15.9  24.84\n",
      " 18.05 30.13 26.35 24.28 15.6  34.15 21.05 27.19 32.78 17.58 22.12 15.75\n",
      " 22.5  25.56 24.91  9.75 22.21 23.8  15.89 32.46 20.35 16.52 19.11 18.73\n",
      " 36.06 20.4  22.64 34.34 23.12 13.95 30.57 15.52 12.59 26.18 27.53 43.09\n",
      " 28.13 25.52 19.23 32.47 19.79 16.6  30.7  24.7  19.98 25.93 26.82 23.47\n",
      " 11.3  25.31 34.03 33.97 14.91 18.77 15.05 18.38 26.45 33.24 12.34 23.03\n",
      " 34.64 14.93 23.28 36.73 17.14 34.7  17.94 15.37 22.67 14.62 29.79 22.81\n",
      " 16.78 32.28 37.26 24.24 25.36 31.14 17.15 27.45 40.67 37.86 12.92 19.81\n",
      " 42.47 18.03 26.74 18.12 34.94 19.64 29.58 17.95 33.04 24.52 22.17 20.63\n",
      " 40.96 29.65 29.6  29.77 28.32 45.26 20.16 27.1  35.15 22.43 25.68 25.06\n",
      " 10.28 20.58 29.18 18.44 21.57 13.94 37.38 19.28 18.7  13.55 28.01 29.17\n",
      " 18.51 21.81 19.63 23.73 27.53 11.45 22.09 15.94 22.45 13.43 11.22 24.36\n",
      " 15.24 20.46 29.94 29.56 14.46 15.01 33.16 32.06 31.92 32.34 21.08 22.05\n",
      " 36.82 20.84 13.05 26.95 23.38 14.55 15.35  9.13 28.94 17.46 37.78 18.16\n",
      " 15.85 14.54 15.66 20.24 40.9  22.22 25.42 23.79 43.73 16.61 27.04 16.21\n",
      " 20.87 31.32 20.79 21.34 19.28 34.06 24.53 11.6  37.24 21.12 20.22 25.02\n",
      " 18.9  17.34 16.92 33.25 27.28 29.24 28.39 17.67 16.28 14.94 32.62 30.79\n",
      " 13.73 20.8  19.04 24.58 23.61 22.29 26.72 29.32 18.17 18.12 34.56 28.08]\n"
     ]
    }
   ],
   "source": [
    "# init yahoo ticker\n",
    "tickers = Ticker(ticker)\n",
    "\n",
    "# obtain ESG data (Yahoo x Sustainalytics)\n",
    "esg_data = tickers.esg_scores\n",
    "df_esg_raw = pd.DataFrame(esg_data)\n",
    "\n",
    "def df_to_numeric(ds):\n",
    "    \"\"\"Converts columns to numeric where possible\"\"\"\n",
    "    try: \n",
    "        return pd.to_numeric(ds)\n",
    "    except:\n",
    "        return ds\n",
    "\n",
    "## wrangle dataframe\n",
    "# replace strings missings with np.nan and bools with 0 or 1\n",
    "# transform s.t. each row represents a firm \n",
    "# convert columns to numeric, where feasible\n",
    "df_esg = df_esg_raw.replace({'No fundamentals data found for any of the summaryTypes=esgScores': np.nan,\n",
    "                            False: 0, \n",
    "                            True: 1})\\\n",
    "                    .T\\\n",
    "                    .apply(df_to_numeric)\n",
    "\n",
    "# store tickers for missing ESG info \n",
    "ticker_missing_esg = df_esg[df_esg.isnull().any(axis=1) == True].index\n",
    "\n",
    "# drop missings\n",
    "df_esg.dropna(inplace=True)\n",
    "\n",
    "print(df_esg.iloc[:,1:5].head())\n",
    "\n",
    "# main outcome, y:total ESG score \n",
    "y = df_esg.totalEsg.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytrends "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inlcude into helper functions\n",
    "# KEYWORD GENERATOR HELPERS \n",
    "def regex_strip_legalname(raw_names):\n",
    "    \"\"\"Removes legal entity, technical description or firm type from firm name\n",
    "    \n",
    "    Input\n",
    "        raw_names: list of strings with firm names\n",
    "        \n",
    "    Return\n",
    "        list of strings: firm names without legal description \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pattern = r\"(,\\s)?(LLC|Inc|Corp\\w*|\\(?Class \\w+\\)?|Group|Company|\\WCo(\\s|\\.)|plc|Ltd|Int'l\\.|Holdings)\\.?\\W?\"\n",
    "    stripped_names = [re.sub(pattern,'', n) for n in raw_names]\n",
    "    \n",
    "    return stripped_names\n",
    "\n",
    "def batch(lst, n=5):\n",
    "    \"\"\"Yield successive n-sized chunks from list lst\n",
    "    \n",
    "    adapted from https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \n",
    "    Input\n",
    "        lst: list \n",
    "        n: selected batch size\n",
    "        \n",
    "    Return \n",
    "        List: lst divided into batches of len(lst)/n lists\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def flatten_list(nested_list):\n",
    "    \"\"\"Flattens nested list\"\"\"\n",
    "    \n",
    "    return [element for sublist in nested_list for element in sublist]\n",
    "\n",
    "def list_remove_duplicates(l):\n",
    "    \"\"\"Removes duplicates from list elements whilst preserving element order\n",
    "    adapted from \n",
    "    https://stackoverflow.com/questions/480214/how-do-you-remove-duplicates-from-a-list-whilst-preserving-order\n",
    "    \n",
    "    Input\n",
    "        list with string elements\n",
    "    \n",
    "    Return \n",
    "        Sorted list without duplicates\n",
    "    \n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in l if not (x in seen or seen_add(x))]\n",
    "\n",
    "\n",
    "\n",
    "# PYTREND HELPERS\n",
    "def pytrends_sleep_init(seconds):\n",
    "    \"\"\"Timeout for certain seconds and re-initialize pytrends\n",
    "    \n",
    "    Input\n",
    "        seconds: int with seconds for timeout\n",
    "        \n",
    "    Return\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"TIMEOUT for {} sec.\".format(seconds))\n",
    "    sleep(seconds)\n",
    "    pt = TrendReq()\n",
    "    \n",
    "def make_x_y_csv(x, y, filename, data_dir):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param y: Data labels\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    \n",
    "    # create dir if nonexistent\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # merge df\n",
    "    y = pd.DataFrame(y)\n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    # export to csv\n",
    "    pd.concat([y, x], axis=1).to_csv(os.path.join(data_dir, filename), \n",
    "                                     header=False, \n",
    "                                     index=False)\n",
    "    \n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))\n",
    "    \n",
    "def make_csv(x, filename, data_dir, append=False, header=False, index=False):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    \n",
    "    # create dir if nonexistent\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # make sure its a df\n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    # export to csv\n",
    "    if not append:\n",
    "        x.to_csv(os.path.join(data_dir, filename), \n",
    "                                     header=header, \n",
    "                                     index=index)\n",
    "    else:\n",
    "        x.to_csv(os.path.join(data_dir, filename),\n",
    "                                     mode = 'a',\n",
    "                                     header=header, \n",
    "                                     index=index)        \n",
    "    \n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer search keywords from firm names and topic\n",
    "\n",
    "**TODO:** def construct_search_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/topics.csv\n",
      "Path created: data/firm_names.csv\n"
     ]
    }
   ],
   "source": [
    "# FIRM NAMES\n",
    "firm_names_raw = list(df_sp500.Security)\n",
    "\n",
    "# remove legal taxonomy and firm type\n",
    "# TODO: add list_remove_duplicates()\n",
    "firm_names = regex_strip_legalname(firm_names_raw)\n",
    "\n",
    "# esg keywords (negative exclusion criteria)\n",
    "topics = ['scandal', 'greenwashing', 'corruption', 'fraud', 'bribe', 'tax', 'forced', 'harassment', 'violation', \n",
    "          'human rights', 'conflict', 'weapons', 'arms trade', 'pollution', 'CO2', 'emission', 'fossil fuel',\n",
    "          'gender inequality', 'discrimination', 'sexism', 'racist', 'intransparent', 'data privacy', 'lawsuit', \n",
    "          'unfair', 'bad', 'problem', 'hate', 'issues', 'controversial']\n",
    "\n",
    "# store lists as csv for retrieval\n",
    "make_csv(topics, 'topics.csv', 'data', append=False, header=True)\n",
    "make_csv(firm_names, 'firm_names.csv', 'data', append=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> Subset for testing purposes\n",
      "Generated 900 keywords for 30 firms and 30 topics each\n",
      "Resulting in 180 queries with 5 keywords each (=batch)\n",
      "\n",
      "Example keyword batch:\n",
      "['scandal 3M ', 'greenwashing 3M ', 'corruption 3M ', 'fraud 3M ', 'bribe 3M ']\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# DEFINE PARAMETERS \n",
    "n_firms = 30\n",
    "batch_size = 5\n",
    "n_keywords = int(n_firms*len(topics))\n",
    "n_query = int(n_keywords/batch_size)\n",
    "n_topics = len(topics)\n",
    "sec_sleep = 45\n",
    "############################\n",
    "\n",
    "\n",
    "# create search keywords as pairwise combintations of firm names + topics\n",
    "search_keywords = [[j+' '+i for j in topics] for i in firm_names]\n",
    "\n",
    "# print(\"{} topic keywords for {} firm each ---> {} pairwise combinations\"\\\n",
    "#       .format(n_topics, n_firms, n_keywords))\n",
    "# print()\n",
    "\n",
    "# Subset for test purposes\n",
    "print(\">>>>>>> Subset for testing purposes\")\n",
    "keywords_sample = search_keywords[:n_firms]\n",
    "print(\"Generated {} keywords for {} firms and {} topics each\".format(n_keywords,n_firms,n_topics))\n",
    "print(\"Resulting in {} queries with {} keywords each (=batch)\".format(n_query, batch_size))\n",
    "\n",
    "## generate keyword batches (= query)\n",
    "# flatten list\n",
    "keyword_batches = flatten_list([list(batch(keywords_sample[i], batch_size)) for i in range(n_firms)])\n",
    "\n",
    "print(\"\\nExample keyword batch:\\n{}\".format(keyword_batches[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/googletrends.csv\n",
      "Payload build for 0. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 1. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 2. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 3. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 4. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 5. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 6. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 7. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 8. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 9. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 10. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 11. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 12. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 13. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 14. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 15. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 16. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 17. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 18. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 19. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 20. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 21. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 22. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 23. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 24. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 25. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 26. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 27. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 28. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 29. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 30. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 31. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 32. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 33. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 34. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 35. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 36. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 37. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 38. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 39. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 40. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 41. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 42. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 43. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 44. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 45. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 46. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 47. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 48. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 49. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 50. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 51. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 52. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 53. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 54. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 55. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 56. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 57. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 58. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 59. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 60. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 61. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 62. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 63. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 64. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 65. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 66. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 67. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 68. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 69. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 70. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 71. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 72. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 73. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 74. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 75. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 76. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 77. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 78. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 79. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 80. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 81. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 82. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 83. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 84. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 85. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 86. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 87. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 88. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 89. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 90. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 91. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 92. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 93. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 94. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 95. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 96. batch\n",
      "Path created: data/googletrends.csv\n",
      "The request failed: Google returned a response with code 429.\n",
      "Query 97 of 180\n",
      "TIMEOUT for 45 sec.\n",
      "RETRY for 97. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 98. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 99. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 100. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 101. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 102. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 103. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 104. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 105. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 106. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 107. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 108. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 109. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 110. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 111. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 112. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 113. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 114. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 115. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 116. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 117. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 118. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 119. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 120. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 121. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 122. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 123. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 124. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 125. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 126. batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/googletrends.csv\n",
      "Payload build for 127. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 128. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 129. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 130. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 131. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 132. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 133. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 134. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 135. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 136. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 137. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 138. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 139. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 140. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 141. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 142. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 143. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 144. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 145. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 146. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 147. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 148. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 149. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 150. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 151. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 152. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 153. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 154. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 155. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 156. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 157. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 158. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 159. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 160. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 161. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 162. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 163. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 164. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 165. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 166. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 167. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 168. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 169. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 170. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 171. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 172. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 173. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 174. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 175. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 176. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 177. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 178. batch\n",
      "Path created: data/googletrends.csv\n",
      "Payload build for 179. batch\n",
      "Path created: data/googletrends.csv\n"
     ]
    }
   ],
   "source": [
    "## retrieve Google trends across time\n",
    "\n",
    "# initialize pytrends\n",
    "pt = TrendReq()\n",
    "\n",
    "# store DFs for later concat\n",
    "df_list = []\n",
    "index_batch_error = []\n",
    "\n",
    "# create csv to store intermediate results\n",
    "make_csv(pd.DataFrame(), filename='googletrends.csv', data_dir='data', append=False)\n",
    "\n",
    "for i, batch in enumerate(keyword_batches):\n",
    "    \n",
    "    # retrieve interest over time\n",
    "    try:\n",
    "        # re-init pytrends and wait (sleep/timeout)\n",
    "        pytrends_sleep_init(sec_sleep)\n",
    "        \n",
    "        # pass keywords to pytrends API\n",
    "        pt.build_payload(kw_list=batch) \n",
    "        print(\"Payload build for {}. batch\".format(i))\n",
    "        df_search_result = pt.interest_over_time()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Query {} of {}\".format(i, n_query))\n",
    "        # store index at which error occurred\n",
    "        index_batch_error.append(i)\n",
    "        \n",
    "        # re-init pytrends and wait (sleep/timeout)\n",
    "        pytrends_sleep_init(sec_sleep)\n",
    "        \n",
    "        # retry\n",
    "        print(\"RETRY for {}. batch\".format(i))\n",
    "        pt.build_payload(kw_list=batch) \n",
    "        df_search_result = pt.interest_over_time()\n",
    "        \n",
    "    # check for non-empty df\n",
    "    if df_search_result.shape[0] != 0:\n",
    "        \n",
    "        # reset index for consistency (to call pd.concat later with empty dfs)\n",
    "        df_search_result.reset_index(inplace=True)\n",
    "        df_list.append(df_search_result)\n",
    "        \n",
    "    # no search result for any keyword\n",
    "    else:        \n",
    "        # create df containing 0s\n",
    "        df_search_result = pd.DataFrame(np.zeros((261,batch_size)), columns=batch)\n",
    "        df_list.append(df_search_result)\n",
    "        \n",
    "    make_csv(df_search_result, filename='googletrends.csv', data_dir='data',\n",
    "             append=True,\n",
    "            header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine query results to df\n",
    "drop_cols = ['isPartial', 'date']\n",
    "\n",
    "# index df\n",
    "df_clean_list = []\n",
    "for i,x in enumerate(range(0,len(df_list),6)):\n",
    "\n",
    "    map_colnames = dict(zip(search_keywords[i+272], list(topics)))\n",
    "    \n",
    "    ## create firm-level df\n",
    "    # df with isPartial and date columns --> drop columns\n",
    "    try:\n",
    "        df_firm = pd.concat(df_list[x:x+6], axis=1).drop(columns=drop_cols)\n",
    "\n",
    "        # rename columns\n",
    "        df_firm.rename(columns=map_colnames, inplace=True)\n",
    "        \n",
    "        # add firm column\n",
    "        df_firm['firm'] = firm_names[i+272]\n",
    "        \n",
    "        df_clean_list.append(df_firm)\n",
    "        \n",
    "    except:\n",
    "        df_firm = pd.concat(df_list[x:x+6], axis=1).rename(columns=map_colnames)\n",
    "        # rename columns\n",
    "        df_firm.rename(columns=map_colnames, inplace=True)\n",
    "        # add firm \n",
    "        df_firm['firm'] = firm_names[i]\n",
    "\n",
    "        df_clean_list.append(df_firm)\n",
    "\n",
    "# df (long format) with time dimension      \n",
    "df_time = pd.concat(df_clean_list)\n",
    "\n",
    "# Store query results so far\n",
    "print('Index batch error:',index_batch_error)\n",
    "\n",
    "# get timestamp\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df_filename = 'df_time_{}_idxbatch_{}.csv'.format(timestr, 1633)\n",
    "print(df_filename)\n",
    "\n",
    "# Store df_time\n",
    "make_csv(df_time, filename=df_filename, data_dir='data', append=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
